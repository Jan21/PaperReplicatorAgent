```json
{
    "section_title": "4 EXPERIMENTAL RESULTS",
    "section_purpose": "To present the empirical evaluation of the proposed neural heuristics, including details on the training datasets, model performance metrics, comparisons with classical SAT heuristics (JW-OS, DLIS), and an ablation study on the attention mechanism.",
    "key_points": [
        "The models were trained on the SR(n) SAT problem class, with separate models for SR(30), SR(50), SR(70), and SR(100), achieving lower sat and policy errors on smaller n.",
        "In DPLL with a 1000-step limit, neural guidance outperformed JW-OS and DLIS on large problems (SR(90), SR(110)), while JW-OS was best on average-sized problems (SR(50), SR(70)).",
        "A hybrid heuristic combining the SR(50) model with JW-OS (switching when model's sat prediction < 0.3^2) solved instances in fewer steps than JW-OS alone in most cases, for both DPLL and CDCL solvers.",
        "The attention mechanism generally improved model performance (lower policy error), except for SR(30) at level 20 where it degraded performance.",
        "Training stability varied; models were excluded if they increased the loss standard deviation above 1, and code with hyperparameters is published for reproducibility."
    ],
    "technical_details": {
        "algorithms": [
            "DPLL algorithm guided by neural models, JW-OS, and DLIS heuristics",
            "Hybrid guidance algorithm: uses neural model (trained on SR(50)) and switches to JW-OS when predicted sat probability < threshold (0.3^2)"
        ],
        "formulas": [
            "Sat probability threshold for hybrid heuristic: 0.3^2",
            "Metrics: sat error = mean absolute error of sat prediction vs label; policy error = mean absolute error of policy prediction vs label"
        ],
        "architectures": [
            "Message-passing neural networks with modified attention mechanism"
        ],
        "hyperparameters": {
            "Batch sizes": "128 for SR(30), 64 for SR(50) and SR(70), 32 for SR(100)",
            "Training steps": "1,200K for SR(30) and SR(100), 600K for SR(50) and SR(70)",
            "Training hardware": "single TPU v2",
            "DPLL step limit": "1000 steps (for Experiment 1)",
            "Evaluation set size": "100 satisfiable SR(n) formulas per experiment"
        },
        "datasets": [
            "SR(n) SAT problem class (from cited work [SLB+18]), parametrized by number of variables n",
            "Balanced in satisfiable/unsatisfiable examples",
            "Each sample has two labels: sat (formula satisfiability) and policy (whether Φ ∧ l is satisfiable for each literal l)",
            "Labels generated by running MiniSat 2.2",
            "Training/evaluation on SR(30), SR(50), SR(70), SR(100), SR(90), SR(110), SR(150)",
            "Sample solving times with MiniSat 2.2 on Xeon E5-2680v3@2.5 GHz: SR(30): 0.007s, SR(110): 0.137s, SR(150): 3.406s"
        ]
    },
    "dependencies": [
        "Section 3 (ARCHITECTURE) for model description and label definitions",
        "References to [SLB+18] for SR(n) dataset details",
        "Knowledge of DPLL and CDCL solving algorithms",
        "Familiarity with JW-OS and DLIS heuristics"
    ],
    "reproducibility_notes": [
        "Training details per model in Table 1: batch size, training steps, training time",
        "Code and hyperparameters published at https://bit.ly/neurheur",
        "Implementation based on TensorFlow, uses CDCL implementation by [Zho18], MiniSat accessed via PySAT interface [IMM18]",
        "Training stability criterion: exclude models that raise loss standard deviation above 1",
        "Evaluation procedure: 100 fresh random satisfiable SR(n) formulas per test, results averaged over 3-5 model replicas",
        "Hybrid heuristic threshold value: 0.3^2"
    ]
}
```